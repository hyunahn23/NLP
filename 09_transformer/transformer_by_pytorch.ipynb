{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhGtK18UtbrS5cx/m9bgR8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# pyTorch로 Transformer 간단 구현"],"metadata":{"id":"lQAXfpHk4OPR"}},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3tIgpFA4C7f","executionInfo":{"status":"ok","timestamp":1744611703573,"user_tz":-540,"elapsed":7460,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"7fdb3573-dd91-4717-ee7b-191853f26f06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bertviz in /usr/local/lib/python3.11/dist-packages (1.4.0)\n","Requirement already satisfied: transformers>=2.0 in /usr/local/lib/python3.11/dist-packages (from bertviz) (4.50.3)\n","Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertviz) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from bertviz) (4.67.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from bertviz) (1.37.33)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bertviz) (2.32.3)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from bertviz) (2024.11.6)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from bertviz) (0.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0->bertviz) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0->bertviz) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.0->bertviz) (0.30.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.0->bertviz) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.0->bertviz) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.0->bertviz) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.0->bertviz) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=2.0->bertviz) (0.5.3)\n","Requirement already satisfied: botocore<1.38.0,>=1.37.33 in /usr/local/lib/python3.11/dist-packages (from boto3->bertviz) (1.37.33)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->bertviz) (1.0.1)\n","Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->bertviz) (0.11.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bertviz) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bertviz) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bertviz) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bertviz) (2025.1.31)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.33->boto3->bertviz) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0->bertviz) (3.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.33->boto3->bertviz) (1.17.0)\n"]}],"source":["!pip install bertviz"]},{"cell_type":"markdown","source":["### 인코더"],"metadata":{"id":"PqtSF58N4rH3"}},{"cell_type":"markdown","source":["##### Self-Attention"],"metadata":{"id":"rFYlLPKP4ve3"}},{"cell_type":"code","source":["# 시각화: 단어와 뉴런 간 연결 분석 시각화 (단일 문장)\n","from transformers import AutoTokenizer\n","from bertviz.transformers_neuron_view import BertModel\n","from bertviz.neuron_view import show\n","\n","model_ckpt = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = BertModel.from_pretrained(model_ckpt)\n","text = \"time flies like an arrow\"\n","show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338,"output_embedded_package_id":"1n6j9TYwNOH8Wpx9vixBu4vif-s8wMBND"},"id":"WusOkAtd43sd","executionInfo":{"status":"ok","timestamp":1744611711257,"user_tz":-540,"elapsed":7683,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"ea2b4b04-ae20-446c-dde5-8a3f41ce7499"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# 시각화: 어텐션 헤드 간의 단어 관계 시각화 (문장 간 관계 포함)\n","from transformers import AutoModel\n","from bertviz import head_view\n","\n","model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n","\n","sent1 = \"time flies like an arrow\"\n","sent2 = \"fruit flies like a banana\"\n","\n","viz_inputs = tokenizer(sent1, sent2, return_tensors=\"pt\")\n","attention = model(**viz_inputs).attentions\n","sent2_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\n","tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n","\n","head_view(attention, tokens, sent2_start, heads=[8])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362,"output_embedded_package_id":"1x-O1PaWGZgJz-pWZNP1-Jqk3rUgU-z5r"},"id":"I4wS4p8BSjNV","executionInfo":{"status":"ok","timestamp":1744611712846,"user_tz":-540,"elapsed":1575,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"35a30aec-1359-4d18-862f-770723a6cc5d"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["BERT 모델\n","(bert-base-uncased) <br>\n","|---- 12개의 레이어 (Transformer Layers) <br>\n","|----|---- 12개의 어텐션 헤드 (각 레이어마다) <br>\n","|----|----|---- 64차원 벡터 출력 (각 헤드마다) <br>\n","|----|----|---- 12개 헤드의 출력을 합쳐 768차원의 뉴런 벡터(hidden_state) 생성"],"metadata":{"id":"zshN7sHaYVcu"}},{"cell_type":"code","source":["# 토큰화\n","inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","inputs.input_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBgl6RjZ8Hle","executionInfo":{"status":"ok","timestamp":1744611712866,"user_tz":-540,"elapsed":3,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"ba280763-b097-4bae-aa20-09067c57d9b4"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2051, 10029,  2066,  2019,  8612]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["# Embedding layer 생성\n","import torch.nn as nn\n","from transformers import AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_ckpt)\n","emb = nn.Embedding(config.vocab_size, config.hidden_size)\n","emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMNq60Gt99jE","executionInfo":{"status":"ok","timestamp":1744611712870,"user_tz":-540,"elapsed":4,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"da30d2fb-84fa-45d9-98f3-490c6ad9d120"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(30522, 768)"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["# Embedding 처리\n","inputs_embedded = emb(inputs.input_ids)\n","inputs_embedded.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j7Arq5ubCCUy","executionInfo":{"status":"ok","timestamp":1744611712885,"user_tz":-540,"elapsed":7,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"c5593a5f-866e-48c8-9476-7f2f776f5276"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# Self-Attention 메커니즘\n","import torch\n","import torch.nn.functional as F\n","from math import sqrt\n","\n","# 1. score 계산\n","query = key = value = inputs_embedded\n","dim_k = key.size(-1)\n","scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","\n","# 2. softmax 적용\n","weights = F.softmax(scores, dim=-1)\n","\n","# 3. attention value 계산\n","attn_outputs = torch.bmm(weights, value)\n","attn_outputs.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRQnYPwXC7dg","executionInfo":{"status":"ok","timestamp":1744611712889,"user_tz":-540,"elapsed":3,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"ad9a2d5d-11bf-48be-b1be-8dc1fe5a13d7"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# Self-Attention 함수\n","def attention(query, key, value):\n","  dim_k = query.size(-1)\n","  scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","  weights = F.softmax(scores, dim=-1)\n","  return torch.bmm(weights, value)"],"metadata":{"id":"-Q0oq1x4F_ek","executionInfo":{"status":"ok","timestamp":1744611712890,"user_tz":-540,"elapsed":1,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# Multi-Head Attention\n","\n","class AttentionHead(nn.Module):\n","  def __init__(self, embed_dim, head_dim):\n","    super().__init__()\n","    self.q = nn.Linear(embed_dim, head_dim)\n","    self.k = nn.Linear(embed_dim, head_dim)\n","    self.v = nn.Linear(embed_dim, head_dim)\n","\n","  def forward(self, hidden_state):\n","    return attention(\n","        self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)\n","    )\n"],"metadata":{"id":"QXi5KAufHcX8","executionInfo":{"status":"ok","timestamp":1744611712892,"user_tz":-540,"elapsed":1,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    embed_dim = config.hidden_size\n","    heads_num = config.num_attention_heads\n","    head_dim = embed_dim // heads_num\n","    self.heads = nn.ModuleList(\n","        [AttentionHead(embed_dim, head_dim) for _ in range(heads_num)]\n","    )\n","    self.output_layer = nn.Linear(embed_dim, embed_dim)\n","\n","  def forward(self, hidden_state):\n","    output = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n","    output = self.output_layer(inputs_embedded)\n","    return output\n"],"metadata":{"id":"gf2eLX7uLeYj","executionInfo":{"status":"ok","timestamp":1744611712903,"user_tz":-540,"elapsed":2,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["multihead_attn = MultiHeadAttention(config)\n","attn_output = multihead_attn(inputs_embedded)\n","attn_output.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKUA1dCJRdGx","executionInfo":{"status":"ok","timestamp":1744611712908,"user_tz":-540,"elapsed":4,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"731af989-55dd-4cf0-fb51-4f4679bdf64e"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["##### Feed-Forward"],"metadata":{"id":"fPDOIEbfxLGj"}},{"cell_type":"markdown","source":["- 간단한 두 개 층으로 구성된 완전 연결 신경망\n","- 전체 임베딩 시퀀스를 하나의 벡터로 처리하지 않고 각 임베딩을 독립적으로 관리\n","- GELU(Gaussian Error Linear Unit) 활성 함수 많이 사용"],"metadata":{"id":"8bF5l14lxW1X"}},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","    self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","    self.gelu = nn.GELU()\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","  def forward(self, x):\n","    x = self.linear1(x)\n","    x = self.gelu(x)\n","    x = self.linear2(x)\n","    x = self.dropout(x)\n","    return x"],"metadata":{"id":"sdiR0IzTxkdz","executionInfo":{"status":"ok","timestamp":1744611712916,"user_tz":-540,"elapsed":9,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["feed_forward = FeedForward(config)\n","ff_outputs = feed_forward(attn_output)\n","ff_outputs.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxN72W3d0h7L","executionInfo":{"status":"ok","timestamp":1744611712921,"user_tz":-540,"elapsed":13,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"2539ad15-9105-4313-f1f2-4cd79bbaef1e"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["##### 정규화를 반영한 Encoder Layer\n","- 사전 층 정규화"],"metadata":{"id":"4iLknVJf2D1J"}},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n","    self.layer_norm2 = nn.LayerNorm(config.hidden_size)\n","    self.attention = MultiHeadAttention(config)\n","    self.feed_forward = FeedForward(config)\n","\n","  def forward(self, x):\n","    hidden_state = self.layer_norm1(x)\n","    x = x + self.attention(hidden_state)\n","    x = x + self.feed_forward(self.layer_norm2(x))\n","    return x"],"metadata":{"id":"yy7pzvZd2Ki1","executionInfo":{"status":"ok","timestamp":1744611712922,"user_tz":-540,"elapsed":3,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["encoder_layer = TransformerEncoderLayer(config)\n","inputs_embedded.shape, encoder_layer(inputs_embedded).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDZ25E-l8Mkg","executionInfo":{"status":"ok","timestamp":1744611712936,"user_tz":-540,"elapsed":15,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"02dc4272-8382-4c65-8fd4-b54fe43299e8"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["##### 위치 임베딩 (Positional Encoding)"],"metadata":{"id":"Y19ZAqj0-PDC"}},{"cell_type":"code","source":["class Embeddings(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n","    self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","    self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n","    self.dropout = nn.Dropout()\n","\n","  def forward(self, inputs_ids):\n","    seq_length = inputs_ids.size(1)\n","    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n","    token_embeddings = self.token_embeddings(inputs_ids)\n","    position_embeddings = self.position_embeddings(position_ids)\n","    embeddings = token_embeddings + position_embeddings\n","    embeddings = self.layer_norm(embeddings)\n","    embeddings = self.dropout(embeddings)\n","    return embeddings"],"metadata":{"id":"bwIc07tl-UVP","executionInfo":{"status":"ok","timestamp":1744613220757,"user_tz":-540,"elapsed":5,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["embedding_layer = Embeddings(config)\n","embedding_layer(inputs.input_ids).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKZ7eSXECI2j","executionInfo":{"status":"ok","timestamp":1744613223003,"user_tz":-540,"elapsed":367,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"c2d882b2-d0df-4817-af63-9b1849f60d8c"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["##### 완전한 인코더"],"metadata":{"id":"-bLs5UX2Cykk"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.embeddings = Embeddings(config)\n","    self.layers = nn.ModuleList([TransformerEncoder(config)\n","                                  for _ in range(config.num_hidden_layers)])\n","\n","  def forward(self, x):\n","    x = self.embeddings(x)\n","    for layer in layers:\n","      x = layer(x)\n","    return x\n"],"metadata":{"id":"bhvBGBKrC0UH","executionInfo":{"status":"ok","timestamp":1744613608558,"user_tz":-540,"elapsed":82,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":["### 디코더"],"metadata":{"id":"5N0rfdZjHhqC"}},{"cell_type":"markdown","source":["- \"Masked\" Multi-Head Self-Attention\n","- Encoder-Decoder Attention"],"metadata":{"id":"O2jh2qaUH8i8"}},{"cell_type":"code","source":["seq_len = inputs.input_ids.size(-1)\n","mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n","mask[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fdea_FtSIDPo","executionInfo":{"status":"ok","timestamp":1744614605123,"user_tz":-540,"elapsed":12,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"eb48dff1-760e-461e-9d45-65fdeba8c86f"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0.],\n","        [1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["scores.masked_fill(mask == 0, -float(\"inf\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VOG2d7pXJafX","executionInfo":{"status":"ok","timestamp":1744614853953,"user_tz":-540,"elapsed":17,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}},"outputId":"e0ad7ca4-982c-4483-b019-78f0507414e2"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 2.5721e+01,        -inf,        -inf,        -inf,        -inf],\n","         [ 8.6981e-03,  2.6122e+01,        -inf,        -inf,        -inf],\n","         [ 2.0405e+00, -1.0714e+00,  2.6252e+01,        -inf,        -inf],\n","         [ 2.6527e-01, -1.6742e+00,  9.3066e-01,  2.6198e+01,        -inf],\n","         [ 7.5245e-01, -8.5750e-01,  1.4154e+00, -6.1670e-01,  2.6905e+01]]],\n","       grad_fn=<MaskedFillBackward0>)"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["# attention + masking\n","def attention(query, key, value, mask=None):\n","  dim_k = query.size(-1)\n","  scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","  if mask is not None:\n","    scores.masked_fill(mask == 0, -float(\"inf\"))\n","  weights = F.softmax(scores, dim=-1)\n","  return torch.bmm(weights, value)"],"metadata":{"id":"kADeHYP9KIW2","executionInfo":{"status":"ok","timestamp":1744615169125,"user_tz":-540,"elapsed":5,"user":{"displayName":"Rabbit","userId":"00154607137017656782"}}},"execution_count":53,"outputs":[]}]}